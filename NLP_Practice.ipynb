{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP Practice",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "955hN08cLt5R"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9zndGlcMT8R",
        "outputId": "c6ce552a-cf3f-4e2e-900b-956d35382070"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_IovH7jMWrw"
      },
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
        "               the world have come and invaded us, captured our lands, conquered our minds. \n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
        "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
        "               We have not grabbed their land, their culture, \n",
        "               their history and tried to enforce our way of life on them. \n",
        "               Why? Because we respect the freedom of others.That is why my \n",
        "               first vision is that of freedom. I believe that India got its first vision of \n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
        "               I see four milestones in my career\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15iCihMXODSM"
      },
      "source": [
        "sentences1 = nltk.sent_tokenize(paragraph)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQapwhWlPCoD",
        "outputId": "c92f1a27-4e4e-484a-e6d6-b71036b15cfe"
      },
      "source": [
        "sentences1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India’s development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K7vxsDcOvZR"
      },
      "source": [
        "words = nltk.word_tokenize(paragraph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZNBhpgBPFlV"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW9v-hrtU6Bi"
      },
      "source": [
        "#Stemming\n",
        "for i in range(len(sentences1)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)   "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTd_fElcfFoy",
        "outputId": "6463658e-2481-499c-b48b-22076fd65bcd"
      },
      "source": [
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I three vision india .',\n",
              " 'In 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'We conquer anyon .',\n",
              " 'We grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'I believ india got first vision 1857 , start war independ .',\n",
              " 'It freedom must protect nurtur build .',\n",
              " 'If free , one respect us .',\n",
              " 'My second vision india ’ develop .',\n",
              " 'for fifti year develop nation .',\n",
              " 'It time see develop nation .',\n",
              " 'We among top 5 nation world term gdp .',\n",
              " 'We 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " 'isn ’ incorrect ?',\n",
              " 'I third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus I believ unless india stand world , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'We must strong militari power also econom power .',\n",
              " 'both must go hand-in-hand .',\n",
              " 'My good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'I lucki work three close consid great opportun life .',\n",
              " 'I see four mileston career']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtXLt8LGk-3x"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatization\n",
        "for i in range(len(sentences1)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set (stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)   "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFaBtPg1pMNd",
        "outputId": "b637313e-22e4-43e2-c416-83cdf9d25716"
      },
      "source": [
        "sentences"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I three vision india .',\n",
              " 'In 3000 year histori , peopl world come invad u , captur land , conquer mind .',\n",
              " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot u , took .',\n",
              " 'yet done nation .',\n",
              " 'We conquer anyon .',\n",
              " 'We grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becau respect freedom others.that first vision freedom .',\n",
              " 'I believ india got first vision 1857 , start war independ .',\n",
              " 'It freedom must protect nurtur build .',\n",
              " 'If free , one respect u .',\n",
              " 'My second vision india ’ develop .',\n",
              " 'fifti year develop nation .',\n",
              " 'It time see develop nation .',\n",
              " 'We among top 5 nation world term gdp .',\n",
              " 'We 10 percent growth rate area .',\n",
              " 'poverti level fall .',\n",
              " 'achiev global recogni today .',\n",
              " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
              " '’ incorrect ?',\n",
              " 'I third vision .',\n",
              " 'india must stand world .',\n",
              " 'becau I believ unless india stand world , one respect u .',\n",
              " 'onli strength respect strength .',\n",
              " 'We must strong militari power also econom power .',\n",
              " 'must go hand-in-hand .',\n",
              " 'My good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'I lucki work three close consid great opportun life .',\n",
              " 'I see four mileston career']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFDM_PdL0dKO"
      },
      "source": [
        "## Bag of words\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJKZ2Lp80mzt"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[a-zA-Z]','', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ''.join(review)\n",
        "  corpus.append(review)\n",
        "  \n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fyFP4Gc60mn"
      },
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRMn9J4b_xge",
        "outputId": "46a2d041-26fa-4a40-d6fb-66e7cfc8ab1f"
      },
      "source": [
        "x"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 1, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6qFP8QHVBeS"
      },
      "source": [
        "###TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9_XzB9lVEHn"
      },
      "source": [
        "wordnet = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "  review = re.sub('[a-zA-Z]','', sentences[i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "  review = ''.join(review)\n",
        "  corpus.append(review)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVXDeXavVPPb"
      },
      "source": [
        "# Creating the TF-IDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCVDcsCXVm_W",
        "outputId": "b2f89662-d061-402a-80c0-4ace588083e0"
      },
      "source": [
        "x"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}